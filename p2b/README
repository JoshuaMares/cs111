NAME: Joshua Moises Mares
EMAIL: joshuamares180@gmail.com
ID: 005154394

lab2_list.c
  This program does the same as lab2_list.c from the previous project but has
  been updated to include a lists option where you indicate the number of lists
  and print the total wait time for locks.

SortedList.h
  Contains the declaration of the never empty doubly linked circular list as
  well as the yield option definitions.

SortedList.c
  Contains the implementation of the list functions.

Makefile
  A makefile to build the programs.  The default option compiles the lab2_list 
  program.  The tests option runs test cases and places them in their
  respective csv files.  The graphs option graphs the csv contents and places
  said graphs in 5 png files.  The dist option compressed all content necessary
  for this project.  The clean option removes all content created by this
  makefile.  The profile option gives us the time profiles for locks and
  functions.

lab2b_list.csv
  Holds the outputs of the test cases for lab2_list.

lab2_list.gp
  Creates the graphs needed for the project.

profile.out
  Time profiles.

raw.gperf
  Contains cpu data needed to create profile.out

*.png
  lab2b_1.png ... throughput vs. number of threads for mutex and spin-lock synchronized list operations.
  lab2b_2.png ... mean time per mutex wait and mean time per operation for mutex-synchronized list operations.
  lab2b_3.png ... successful iterations vs. threads for each synchronization method.
  lab2b_4.png ... throughput vs. number of threads for mutex synchronized partitioned lists.
  lab2b_5.png ... throughput vs. number of threads for spin-lock-synchronized partitioned lists.

test.sh
  Script that conducts the test cases.

Questions:
2.3.1
  For 1 and 2 thread list tests most of the time is spent actually running the
  critical section.  The actual list operations are taking up the most resources
  since we are not wasting time waiting on spin locks and mutex locks.  For
  higher numbers of threads, the spring lock cpu cycles are being wasted waiting
  for the spin lock to be released.  For higher numbers of threads, the mutex
  option spends most of its time performing context switchs.

2.3.2
  The spin lock loop is consuming most of the cycles.  This operation becomes
  more expensive with larger numbers of threads because rather than waiting for
  one other thread to finish, each thread is wasting cpu cycles waiting for
  every other thread to finish.

2.3.3
  Average lock time increases drastically because each thread waits for an
  increasing amount of time for each thread.  Average completion time per
  operation increases less drastically since at least one thread is able to do
  work so the total completion time is divided by 3.  Wait time per operation
  increases quicker than completion time per operation due to redundancy in the
  total.

2.3.4
  As the number of lists increase, so does the aggregated throughput.  This is
  because each thread only has to wait for a specific list that most likely is
  not being used by another thread at the same time.  This allows each thread to
  be able to do more work without waiting on locks.  The throughput should
  increase up to the point where the number of lists equals or exceeds the
  number of possible hashes.  In my program I moduloed the first character in
  the key by the number of lists.  Given my range of 0-z ascii, this gives me 75
  hashes, which is a little under the total amount of ascii printable characters.
  Once the number of lists exceeds the number of hashes we have a list for each
  character and given a randomized input, lock wait time should be almost
  non-existant.  The throughput of a n-way partitioned list should be equivalent
  to a single list with less threads.  This can be seen in graph 4 but not so
  so much graph 5 but that is most likely due to some error I cannot find.
  Performance from graph 5 should be similar to graph 4 albeit a little worse.

Sources:
  Man Pages
  Piazza Discussions
  Discussion Section
