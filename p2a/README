NAME: Joshua Moises Mares
EMAIL: joshuamares180@gmail.com
ID: 005154394

lab2_add.c
  This program adds to a counter via multithreading.  The --threads=t argument
  creates t threads and has a default value of 1.  The --iterations=i adds and
  subtracts from the counter i number of times each and has a default value of
  1.  The --yield option forces a yield at the worst possible time to maximize
  the number of race conditions.  The --sync={msc} synchronizes the threads in
  one of 3 ways: m)mutexes s)spin locks c)compare and swap.  When the program
  finishes it prints out the name of the test, number of threads, number of
  iterations, number of operations performed, total runtime in nanoseconds, the
  average time per operation in nanoseconds, and the total of the counter at
  the end of the program.

lab2_list.c
  This program does the same as lab2_add.c but with a never empty doubly linked
  circular list.  The threads and iterations argument are the same.  The
  --sync{ms} option now only supports mutexes and spin locks.  The --yield={idl}
  option can now take multiple arguments.  The i argument schedules a yield in
  the insert function, the d argument in the delete function, and the l argument
  in the lookup and list length functions.  Yield accepts any number and
  combination of these arguments.

SortedList.h
  Contains the declaration of the never empty doubly linked circular list as
  well as the yield option definitions.

SortedList.C
  Contains the implementation of the list functions.

Makefile
  A makefile to build the programs.  The default option compile the lab2_add and
  lab2_list programs.  The tests option runs test cases and places them in their
  respective csv files.  The graphs option graphs the csv contents and places
  said graphs in 9 png files.  The dist option compressed all content necessary
  for this project.  The clean option removes all content created by this
  makefile.

lab2_add.csv
  Holds the outputs of the test cases for lab2_add.

lab2_list.csv
  Holds the outputs of the test cases for lab2_list.

*.png
  lab2_add-1.png ... threads and iterations required to generate a failure (with and without yields)
  lab2_add-2.png ... average time per operation with and without yields.
  lab2_add-3.png ... average time per (single threaded) operation vs. the number of iterations.
  lab2_add-4.png ... threads and iterations that can run successfully with yields under each of the synchronization options.
  lab2_add-5.png ... average time per (protected) operation vs. the number of threads.
  lab2_list-1.png ... average time per (single threaded) unprotected operation vs. number of iterations (illustrating the correction of the per-operation cost for the list length).
  lab2_list-2.png ... threads and iterations required to generate a failure (with and without yields).
  lab2_list-3.png ... iterations that can run (protected) without failure.
  lab2_list-4.png ... (length-adjusted) cost per operation vs the number of threads for the various synchronization options.

test.sh
  Script that conducts the test cases.

Questions:
2.1.1
  It takes many iterations before errors are seen because the probability of a
  race condition is very low.  In addition, most of the time spent in the
  program is dedicated to the creation of threads.  Compared to the time
  necessary to create a thread, the add operation costs next to nothing, so the
  first thread created often gets free reign on the variable for which there is
  a race condition.  When we lower the number of iterations, the total cost of
  the add functions becomes even smaller and thus less likely.

2.1.2
  Yield runs are much slower because of the context switching that occurs on
  every iteration.  Using the code provided, we would be unable to accurately
  measure the per-operation timings when using the yield option.  We would have
  get the time before and after each yield and account for them later.  This,
  however, would lead to another race condition.

2.1.3
  The average cost per operation drops with increasing iterations because the
  time needed to create the threads is included in this measurement.  Since the
  thread creation time cost is high compared to the per iteration time cost,
  with only a few iterations the average will be heavily skewed to the average
  time it takes to create a thread.  However when iterations are high, the
  average will be more indicative of the time cost per iteration, which is low.
  If we run the program with a large enough number of iterations, we will
  eventually come to the exact or correct cost per iteration.

2.1.4
  For a low number of threads, all options perform similarly because the wait
  time for locks is almost non-existent.  So the average time costs depend on
  the creation of threads which is the same for all options.  As the number of
  threads rises, the queue to receive locks becomes larger and thus each thread
  must wait longer.

2.2.1
  For the add program, the time cost per mutex protected operation barely
  increases to the point where the graph almost looks flat. For the list
  program, however, it is increasing at a higher rate.  This is most likely
  because the mutex locks for the list program are held for longer than the add
  program.

2.2.2
  For spin locks, both programs have a quickly increasing time per protected
  operation.  However, the list program has an almost exponential looking curve
  as we increase the number of threads.  This is due to the already horrible
  inneficiency of the spin lock being combined with longer lock times.


Sources:
  Man Pages
  Piazza Discussions
  Discussion Section
